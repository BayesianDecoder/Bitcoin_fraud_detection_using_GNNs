{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":584717,"sourceType":"datasetVersion","datasetId":284040}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install networkx\n!pip install torch-geometric\n!pip install node2vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T21:15:09.183101Z","iopub.execute_input":"2025-05-19T21:15:09.183376Z","iopub.status.idle":"2025-05-19T21:15:47.943508Z","shell.execute_reply.started":"2025-05-19T21:15:09.18335Z","shell.execute_reply":"2025-05-19T21:15:47.942471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------- #\n# General Kaggle imports #\n# ---------------------- #\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'): \n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd        \n\n\n# ------------------ #\n# Additional imports #\n# ------------------ #\nimport random\nimport torch\nimport torch_geometric\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport community as community_louvain\nimport torch.nn.functional as F\n\n\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv, GATConv\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score,\n    confusion_matrix, \n    classification_report,\n    ConfusionMatrixDisplay\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch_geometric.data import Data \nfrom scipy.stats import ttest_ind\n\nprint(\"Torch version:\", torch.__version__)\nprint(\"Torch Geometric version:\", torch_geometric.__version__)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=mpl.MatplotlibDeprecationWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-05-19T21:15:47.946298Z","iopub.execute_input":"2025-05-19T21:15:47.946985Z","iopub.status.idle":"2025-05-19T21:15:53.466501Z","shell.execute_reply.started":"2025-05-19T21:15:47.946943Z","shell.execute_reply":"2025-05-19T21:15:53.465628Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:53.46754Z","iopub.execute_input":"2025-05-19T21:15:53.468068Z","iopub.status.idle":"2025-05-19T21:15:54.530637Z","shell.execute_reply.started":"2025-05-19T21:15:53.468035Z","shell.execute_reply":"2025-05-19T21:15:54.529741Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<img src=\"https://www.bitcoinsistemi.com/wp-content/uploads/2023/12/bitcoin.jpg\" alt=\"Bitcoin Graph Image\" width=\"800\"/>\n\n# üí∞Bitcoin Transactions üîó Graph Neural Networks ü™ô\n\nThis notebook tackles [Kaggles Elliptic Bitcoin Transaction Dataset](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set/data) which maps Bitcoin transactions to real entities. In the description the objective for this dataset is described as:\n> classifying the illicit and licit nodes in the graph.\n\n[Wikipedia describes Bitcoin](https://en.wikipedia.org/wiki/Bitcoin) as the first decentralized cryptocurrency. It also states that nodes in the Bitcoin network verify transactions through cryptography and record them in a public distributed ledger, called _blockchain_. The dataset we are going to work on in this notebook is an anonymized transaction graph collexted from such Bitcoin blockchain. Kaggles Data Card describes the dataset as having the following content: \n\n__Dataset Content__\n- _Node_: represents a transaction. \n    - Each has 166 features\n    - Labeled as ‚úÖ _licit_, ‚ùå _illicit_ or ü§∑ _unknown_\n- _Edge_: represents a flow of Bitcoins between transaction `A` and transaction `B` \n\nBefore we start, here are a couple concepts you should know for better understanding of the following code: \n\n| Concept | Math Notation | Description | More Information | \n| ------- | ------------- | ----------- | ---------------- |\n| Graph | $G$ | Defined as pair $G=(V,E)$, where $V$ is a set of vertices and $E$ is a set of edges which are unordered pairs ${v_1, v_2}$ of vertices. | [Wikipedia: Graph (discrete mathematics)](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) | \n| Degree | $deg$ | Number of edges that are incident to a vertex. | [Wikipedia: Degree (graph theory)](https://en.wikipedia.org/wiki/Degree_(graph_theory)) | \n| Component | | Connected subgraph that is not part of any larger connected subgraph. | [Wikipedia: Component (graph theory](https://en.wikipedia.org/wiki/Component_(graph_theory)) | \n\nüö® In the following notebook we will always call the graph $G$, the set of nodes $V$ and the set of edges $E$. üö®","metadata":{}},{"cell_type":"markdown","source":"# ‚öôÔ∏è Global Settings","metadata":{}},{"cell_type":"code","source":"RANDOM_STATE = 42\nNUM_EPOCHS = 100","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:54.531962Z","iopub.execute_input":"2025-05-19T21:15:54.532273Z","iopub.status.idle":"2025-05-19T21:15:54.536752Z","shell.execute_reply.started":"2025-05-19T21:15:54.532245Z","shell.execute_reply":"2025-05-19T21:15:54.535894Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed_for_torch(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)      # For single-GPU.\n        torch.cuda.manual_seed_all(seed)  # For multi-GPU.\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \ndef set_seed_for_numpy(seed):\n    np.random.seed(seed) \n    \ndef set_seed_for_random(seed):\n    random.seed(seed)    ","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:54.538032Z","iopub.execute_input":"2025-05-19T21:15:54.538647Z","iopub.status.idle":"2025-05-19T21:15:54.552503Z","shell.execute_reply.started":"2025-05-19T21:15:54.538602Z","shell.execute_reply":"2025-05-19T21:15:54.551647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"set_seed_for_torch(RANDOM_STATE)\nset_seed_for_numpy(RANDOM_STATE)\nset_seed_for_random(RANDOM_STATE)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:54.553746Z","iopub.execute_input":"2025-05-19T21:15:54.554017Z","iopub.status.idle":"2025-05-19T21:15:54.571206Z","shell.execute_reply.started":"2025-05-19T21:15:54.553994Z","shell.execute_reply":"2025-05-19T21:15:54.570416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üõ†Ô∏è Helper Methods","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------- #\n# Training, Evaluation and prediction methods #\n# ------------------------------------------- #\n\ndef train(model, data, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data)\n    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n\ndef evaluate(model, data, mask):\n    model.eval()\n    with torch.no_grad():\n        out = model(data)\n        pred = out.argmax(dim=1)\n        \n        correct = (pred[mask] == data.y[mask]).sum().item()\n        accuracy = correct / mask.sum().item()\n\n        y_true = data.y[mask].cpu().numpy()\n        y_pred = pred[mask].cpu().numpy()\n\n        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n\n        metrics = {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1\n        }\n\n    return metrics\n\ndef predict(model, data):\n    model.eval()\n    with torch.no_grad():\n        out = model(data)\n        pred = out.argmax(dim=1)\n    return pred\n\ndef predict_probabilities(model, data):\n    model.eval()\n    with torch.no_grad():\n        out = model(data)\n        probabilities = torch.exp(out)\n    return probabilities","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:54.574769Z","iopub.execute_input":"2025-05-19T21:15:54.575057Z","iopub.status.idle":"2025-05-19T21:15:54.583258Z","shell.execute_reply.started":"2025-05-19T21:15:54.575033Z","shell.execute_reply":"2025-05-19T21:15:54.582439Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_gnn(num_epochs, data, model, optimizer, criterion):\n    \n    train_losses = []\n    train_accuracies = []\n    train_precisions = []\n    train_recalls = []\n    train_f1_scores = []\n\n    val_accuracies = []\n    val_precisions = []\n    val_recalls = []\n    val_f1_scores = []\n\n    # ----- #\n    # Train #\n    # ----- #\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(data)\n        loss = criterion(out[data.train_mask], \n                         data.y[data.train_mask])\n        loss.backward()\n        optimizer.step()\n\n        # --- Calculate training metrics ---\n        pred_train = out[data.train_mask].argmax(dim=1)\n        correct_train = (pred_train == data.y[data.train_mask]).sum()\n        train_acc = int(correct_train) / int(data.train_mask.sum())\n        train_accuracies.append(train_acc)\n\n        y_true_train = data.y[data.train_mask].cpu().numpy()\n        y_pred_train = pred_train.cpu().numpy()\n\n        train_prec = precision_score(y_true_train, y_pred_train, average='weighted', zero_division=0)\n        train_rec = recall_score(y_true_train, y_pred_train, average='weighted', zero_division=0)\n        train_f1 = f1_score(y_true_train, y_pred_train, average='weighted', zero_division=0)\n\n        train_precisions.append(train_prec)\n        train_recalls.append(train_rec)\n        train_f1_scores.append(train_f1)\n        train_losses.append(loss.item())\n\n        # --- Validate and calculate validation metrics ---\n        model.eval()\n        with torch.no_grad():\n            out = model(data)\n            pred_val = out[data.val_mask].argmax(dim=1)\n            correct_val = (pred_val == data.y[data.val_mask]).sum()\n            val_acc = int(correct_val) / int(data.val_mask.sum())\n            val_accuracies.append(val_acc)\n\n            y_true_val = data.y[data.val_mask].cpu().numpy()\n            y_pred_val = pred_val.cpu().numpy()\n\n            val_prec = precision_score(y_true_val, y_pred_val, average='weighted', zero_division=0)\n            val_rec = recall_score(y_true_val, y_pred_val, average='weighted', zero_division=0)\n            val_f1 = f1_score(y_true_val, y_pred_val, average='weighted', zero_division=0)\n\n            val_precisions.append(val_prec)\n            val_recalls.append(val_rec)\n            val_f1_scores.append(val_f1)\n\n        if epoch % 10 == 0:        \n            print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Train - Acc: {train_acc:.4f} - Prec: {train_prec:.4f} - Rec: {train_rec:.4f} - F1: {train_f1:.4f}')\n            print(f'                         Val   - Acc: {val_acc:.4f} - Prec: {val_prec:.4f} - Rec: {val_rec:.4f} - F1: {val_f1:.4f}')        \n\n    return {\n        'train': {\n            'losses': train_losses,\n            'accuracies': train_accuracies,\n            'precisions': train_precisions,\n            'recalls': train_recalls,\n            'f1_scores': train_f1_scores,\n        },\n        'val': {\n            'accuracies': val_accuracies,\n            'precisions': val_precisions,\n            'recalls': val_recalls,\n            'f1_scores': val_f1_scores,            \n        }\n    }","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:54.584387Z","iopub.execute_input":"2025-05-19T21:15:54.5847Z","iopub.status.idle":"2025-05-19T21:15:54.60478Z","shell.execute_reply.started":"2025-05-19T21:15:54.584664Z","shell.execute_reply":"2025-05-19T21:15:54.604064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_train_val_test_metrics(train_val_metrics, test_metrics, num_epochs):\n    plt.figure(figsize=(10, 6))\n\n    # --- Accuracy ----\n    plt.subplot(2, 2, 1)\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['train']['accuracies'], color='C0', linewidth=1.0, label='Training')\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['val']['accuracies'], color='C1', linewidth=1.0, label='Validation', linestyle=':')\n    plt.axhline(y=test_metrics['accuracy'], color='C2', linewidth=0.5, linestyle='--', label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(fontsize=8)\n    plt.title('Accuracy')\n\n    # --- Precision ---\n    plt.subplot(2, 2, 2)\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['train']['precisions'], color='C0', linewidth=1.0, label='Training')\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['val']['precisions'], color='C1', linewidth=1.0, label='Validation', linestyle=':')\n    plt.axhline(y=test_metrics['precision'], color='C2', linewidth=0.5, linestyle='--', label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Precision')\n    plt.legend(fontsize=8)\n    plt.title('Precision')\n\n    # --- Recall ---\n    plt.subplot(2, 2, 3)\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['train']['recalls'], color='C0', linewidth=1.0, label='Training')\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['val']['recalls'], color='C1', linewidth=1.0, label='Validation', linestyle=':')\n    plt.axhline(y=test_metrics['recall'], color='C2', linewidth=0.5, linestyle='--', label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Recall')\n    plt.legend(fontsize=8)\n    plt.title('Recall')\n\n    # --- F1-Score ---\n    plt.subplot(2, 2, 4)\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['train']['f1_scores'], linewidth=1.0, color='C0', label='Training')\n    plt.plot(range(1, num_epochs + 1), train_val_metrics['val']['f1_scores'], linewidth=1.0, color='C1', label='Validation', linestyle=':')\n    plt.axhline(y=test_metrics['f1_score'], color='C2', linewidth=0.5, linestyle='--', label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('F1-Score')\n    plt.legend(fontsize=8)\n    plt.title('F1-Score')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:54.605613Z","iopub.execute_input":"2025-05-19T21:15:54.605837Z","iopub.status.idle":"2025-05-19T21:15:54.620989Z","shell.execute_reply.started":"2025-05-19T21:15:54.605814Z","shell.execute_reply":"2025-05-19T21:15:54.620213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìú Overview\n\nLet's start by getting an overview of the datasets.","metadata":{}},{"cell_type":"code","source":"elliptic_txs_features = pd.read_csv('/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\nelliptic_txs_classes = pd.read_csv('/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\nelliptic_txs_edgelist = pd.read_csv('/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n\nelliptic_txs_features.columns = ['txId'] + [f'V{i}' for i in range(1, 167)]\n\nprint(f\"\"\"Shapes\n{4*' '}Features : {elliptic_txs_features.shape[0]:8,} (rows)  {elliptic_txs_features.shape[1]:4,} (cols)\n{4*' '}Classes  : {elliptic_txs_classes.shape[0]:8,} (rows)  {elliptic_txs_classes.shape[1]:4,} (cols)\n{4*' '}Edgelist : {elliptic_txs_edgelist.shape[0]:8,} (rows)  {elliptic_txs_edgelist.shape[1]:4,} (cols)\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:15:54.622058Z","iopub.execute_input":"2025-05-19T21:15:54.62242Z","iopub.status.idle":"2025-05-19T21:16:10.919986Z","shell.execute_reply.started":"2025-05-19T21:15:54.622394Z","shell.execute_reply":"2025-05-19T21:16:10.918911Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Nodes & Edges__\n\n- $|V| = 203,768$ (number of nodes in $G$)\n- $|E| = 234,355$ (number of edges in $G$)","metadata":{}},{"cell_type":"code","source":"elliptic_txs_features.head(3)  # Dataset containing the node features.","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:10.92125Z","iopub.execute_input":"2025-05-19T21:16:10.921551Z","iopub.status.idle":"2025-05-19T21:16:10.960578Z","shell.execute_reply.started":"2025-05-19T21:16:10.921522Z","shell.execute_reply":"2025-05-19T21:16:10.959673Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elliptic_txs_classes.head(3)  # Dataset containing the classes of the node.","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:10.961796Z","iopub.execute_input":"2025-05-19T21:16:10.962198Z","iopub.status.idle":"2025-05-19T21:16:10.970752Z","shell.execute_reply.started":"2025-05-19T21:16:10.962162Z","shell.execute_reply":"2025-05-19T21:16:10.969692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elliptic_txs_edgelist.head(3)  # Dataset containing the edges.","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:10.971864Z","iopub.execute_input":"2025-05-19T21:16:10.972107Z","iopub.status.idle":"2025-05-19T21:16:10.983308Z","shell.execute_reply.started":"2025-05-19T21:16:10.972083Z","shell.execute_reply":"2025-05-19T21:16:10.98253Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's get an overview on the distribution of the `class`es. ","metadata":{}},{"cell_type":"code","source":"elliptic_txs_classes['class_mapped'] = elliptic_txs_classes['class'].replace({'1': 'illicit', '2': 'licit'})\n\npercentage_distribution = round(100 * elliptic_txs_classes['class_mapped'].value_counts(normalize=True), 2)\nclass_counts = elliptic_txs_classes['class_mapped'].value_counts()\n\nemoji_mapping = {'licit': '‚úÖ', 'illicit': '‚ùå', 'unknown': 'ü§∑'}\nelliptic_txs_classes['emoji'] = elliptic_txs_classes['class_mapped'].map(emoji_mapping)\n\nclasses_df = pd.DataFrame({\n    'Class Mapped': elliptic_txs_classes['class_mapped'].unique(),\n    'Class Raw': elliptic_txs_classes['class'].unique(),    \n    'Counts': class_counts.values,\n    'Percentage': percentage_distribution.values,\n    'Emoji': [emoji_mapping[class_label] for class_label in elliptic_txs_classes['class_mapped'].unique()]\n})\nclasses_df","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:10.984276Z","iopub.execute_input":"2025-05-19T21:16:10.984514Z","iopub.status.idle":"2025-05-19T21:16:11.094097Z","shell.execute_reply.started":"2025-05-19T21:16:10.98449Z","shell.execute_reply":"2025-05-19T21:16:11.093104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 77.15% of all classes are `unknown`\n- 20.62% of all classes are `licit` (2)\n- 2.23% of all classes are `illicit` (1)","metadata":{}},{"cell_type":"code","source":"num_nodes = elliptic_txs_features.shape[0]\nnum_edges = elliptic_txs_edgelist.shape[0]\n\nprint(f\"Number of nodes: {num_nodes:,}\")\nprint(f\"Number of edges: {num_edges:,}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:11.095224Z","iopub.execute_input":"2025-05-19T21:16:11.095487Z","iopub.status.idle":"2025-05-19T21:16:11.100475Z","shell.execute_reply.started":"2025-05-19T21:16:11.095462Z","shell.execute_reply":"2025-05-19T21:16:11.099297Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create graph from the edgelist.\nG = nx.from_pandas_edgelist(elliptic_txs_edgelist, 'txId1', 'txId2')","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:11.101567Z","iopub.execute_input":"2025-05-19T21:16:11.101878Z","iopub.status.idle":"2025-05-19T21:16:12.048476Z","shell.execute_reply.started":"2025-05-19T21:16:11.101843Z","shell.execute_reply":"2025-05-19T21:16:12.047505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sampling for easier analysis...for now.\nclasses_sampled = elliptic_txs_classes.groupby('class_mapped').sample(frac=0.05, random_state=RANDOM_STATE)\ntxIds_sampled = classes_sampled['txId']\n\n# Filter elliptic_txs_edgelist based on the sampled txIds\nedgelist_sampled = elliptic_txs_edgelist[\n    elliptic_txs_edgelist['txId1'].isin(txIds_sampled) | elliptic_txs_edgelist['txId2'].isin(txIds_sampled)\n]\n\n# Filter elliptic_txs_features based on the sampled txIds\nfeatures_sampled = elliptic_txs_features[elliptic_txs_features['txId'].isin(txIds_sampled)]\n\nprint(f\"Sampled Classes:\\n{classes_sampled['class_mapped'].value_counts()}\\n\")\nprint(f\"Sampled Edgelist: {edgelist_sampled.shape[0]:,}\")\nprint(f\"Sampled Features: {features_sampled.shape[0]:,}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:12.049905Z","iopub.execute_input":"2025-05-19T21:16:12.05037Z","iopub.status.idle":"2025-05-19T21:16:12.123769Z","shell.execute_reply.started":"2025-05-19T21:16:12.050322Z","shell.execute_reply":"2025-05-19T21:16:12.122919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç Exploratory Data Analysis (EDA) ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4, 4))\nsizes = [num_nodes, num_edges]\nlabels = ['Nodes', 'Edges']\ncolors = ['skyblue', 'salmon']\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, \n        wedgeprops = {'edgecolor' : 'black', \n                      'linewidth': 2, \n                      'antialiased': True})\nplt.title('Basic Graph Properties')\nplt.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:12.124792Z","iopub.execute_input":"2025-05-19T21:16:12.125066Z","iopub.status.idle":"2025-05-19T21:16:12.284561Z","shell.execute_reply.started":"2025-05-19T21:16:12.125039Z","shell.execute_reply":"2025-05-19T21:16:12.28246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Degree Distribution (Log-Log Scale) ---\nplt.figure(figsize=(6, 4))\ndegrees = [G.degree(n) for n in G.nodes()]\nplt.hist(degrees, bins=50, log=True, color='skyblue', edgecolor='black', linewidth=2.0)\nplt.xscale('log')\nplt.yscale('log')\nplt.title('Degree Distribution (Log-Log Scale)')\nplt.xlabel('Degree (log)')\nplt.ylabel('Number of Nodes (log)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:12.287404Z","iopub.execute_input":"2025-05-19T21:16:12.287859Z","iopub.status.idle":"2025-05-19T21:16:13.657586Z","shell.execute_reply.started":"2025-05-19T21:16:12.287804Z","shell.execute_reply":"2025-05-19T21:16:13.65664Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Basic Statistics","metadata":{}},{"cell_type":"code","source":"# Number of nodes and edges\nnum_nodes = features_sampled.shape[0]\nnum_edges = edgelist_sampled.shape[0]\n\nprint(f\"Number of nodes: {num_nodes:,}\")\nprint(f\"Number of edges: {num_edges:,}\")\n\n# Distribution of node degrees\nplt.figure(figsize=(7, 4))\n\nnode_degrees = edgelist_sampled['txId1'].value_counts() + edgelist_sampled['txId2'].value_counts()\nnode_degrees.hist(bins=50, edgecolor='black')\nplt.title('Distribution of Node Degrees')\nplt.xlabel('Degree')\nplt.ylabel('Number of Nodes')\nplt.grid(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:13.658762Z","iopub.execute_input":"2025-05-19T21:16:13.659084Z","iopub.status.idle":"2025-05-19T21:16:13.978991Z","shell.execute_reply.started":"2025-05-19T21:16:13.659054Z","shell.execute_reply":"2025-05-19T21:16:13.978186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîó Graph Connectivity","metadata":{}},{"cell_type":"code","source":"# Create graph from the edgelist.\nG = nx.from_pandas_edgelist(edgelist_sampled, 'txId1', 'txId2')","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:13.98035Z","iopub.execute_input":"2025-05-19T21:16:13.98076Z","iopub.status.idle":"2025-05-19T21:16:14.036393Z","shell.execute_reply.started":"2025-05-19T21:16:13.980709Z","shell.execute_reply":"2025-05-19T21:16:14.035702Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Connected components ---\nnum_connected_components = nx.number_connected_components(G)\nprint(f\"Number of connected components: {num_connected_components}\")\n\n# --- Giant component analysis ---\ngiant_component = max(nx.connected_components(G), key=len)\nG_giant = G.subgraph(giant_component)\n\nprint(f\"Giant component - Number of nodes: {G_giant.number_of_nodes():,}\")\nprint(f\"Giant component - Number of edges: {G_giant.number_of_edges():,}\")\n\n\n# --------------------------------------------------------- #\n# Shortest paths length distribution in the giant component #\n# --------------------------------------------------------- #\npath_lengths = dict(nx.shortest_path_length(G_giant))\npath_lengths_values = [length for target_lengths in path_lengths.values() for length in target_lengths.values()]\n\nplt.figure(figsize=(7, 4))\nplt.hist(path_lengths_values, bins=50, edgecolor='black')\nplt.title('Shortest Path Length Distribution in Giant Component')\nplt.xlabel('Path Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:14.042362Z","iopub.execute_input":"2025-05-19T21:16:14.04263Z","iopub.status.idle":"2025-05-19T21:16:15.316275Z","shell.execute_reply.started":"2025-05-19T21:16:14.042602Z","shell.execute_reply":"2025-05-19T21:16:15.315197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéØ Node Centrality Measures\n\nIn this section we are going to investigate node centrality measures. Such measures are used in graph theory to identify the most important or influential nodes within a network. We will focus on the following three centrality measures: \n\n___üî¢ Degree Centrality___\n\n**Definition:**\nDegree centrality measures the number of direct connections a node has in a network. It is defined as the number of edges connected to a node.\n\n**Mathematical Function:**\n$$\n\\begin{aligned}\n& C_D(v) = \\frac{\\text{deg}(v)}{N - 1} \\\\ \n& \\text{Where:} \\\\\n& \\qquad \\text{deg}(v) \\; \\text{is the degree of node} \\; (v) \\; \\text{(i.e., the number of edges connected to the node).} \\\\\n& \\qquad N \\; \\text{is the total number of nodes in the network.}\n\\end{aligned}\n$$\n\n**Interpretation:**\nA node with a high degree centrality is highly connected and may play a crucial role in the network by having direct interactions with many other nodes.\n\n___üîÄ Betweenness Centrality___\n\n**Definition:**\nBetweenness centrality measures the extent to which a node lies on the shortest paths between other nodes in the network.\n\n**Mathematical Function:**\n$$\n\\begin{aligned}\n& C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}} \\\\\n& \\text{Where:} \\\\\n& \\qquad \\sigma_{st} \\; \\text{is the total number of shortest paths between nodes} \\; (s) \\; \\text{and} \\; (t). \\\\\n& \\qquad \\sigma_{st}(v) \\; \\text{is the number of those paths that pass through node} \\; (v).\n\\end{aligned}\n$$\n\n**Interpretation:**\nA node with high betweenness centrality has significant control over the flow of information or resources in the network because it connects different parts of the network. It often indicates a node that is critical for communication.\n\n___üåê Closeness Centrality___\n\n**Definition:**\nCloseness centrality measures how close a node is to all other nodes in the network. It is the reciprocal of the sum of the shortest path distances from the node to all other nodes in the network.\n\n**Mathematical Function:**\n$$\n\\begin{aligned}\n& C_C(v) = \\frac{N-1}{\\sum_{t \\neq v} d(v, t)} \\\\\n& \\text{Where:} \\\\ \n& \\qquad d(v, t) \\; \\text{is the shortest path distance between node} \\; (v) \\; \\text{and node} \\; (t). \\\\\n& \\qquad N \\; \\text{is the total number of nodes in the network.}\n\\end{aligned}\n$$\n\n**Interpretation:**\nA node with high closeness centrality can quickly interact with all other nodes and can be an effective spreader of information or influence throughout the network.\n\n---\n","metadata":{}},{"cell_type":"code","source":"# Degree centrality.\ndegree_centrality = nx.degree_centrality(G_giant)\ntop_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\ndf_top_degree_centrality = pd.DataFrame(top_degree_centrality, columns=['Node', 'Degree Centrality'])\n\nprint(\"Top 10 nodes by degree centrality:\")\nprint(\"==================================\")\nprint(df_top_degree_centrality)\n\ntop_nodes_by_ = df_top_degree_centrality['Node'].tolist()\nsubgraph = G_giant.subgraph(top_nodes_by_)\nnode_color = [degree_centrality[node] for node in subgraph.nodes()]\nnorm = mpl.colors.Normalize(vmin=min(node_color), vmax=max(node_color))\nnode_color_normalized = [norm(value) for value in node_color]\ncmap = plt.cm.cool\n\n# -------- #\n# Plotting #\n# -------- #\nplt.figure(figsize=(7, 3))\n\nnx.draw(subgraph, with_labels=True, node_size=300, edge_color='gray', font_size=8, \n        node_color=node_color_normalized, cmap=cmap)\nplt.title('Top 10 Degree Centrality Nodes')\nplt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), label='Degree Centrality')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:15.317402Z","iopub.execute_input":"2025-05-19T21:16:15.317653Z","iopub.status.idle":"2025-05-19T21:16:15.625752Z","shell.execute_reply.started":"2025-05-19T21:16:15.317627Z","shell.execute_reply":"2025-05-19T21:16:15.624879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Betweenness centrality.\nbetweenness_centrality = nx.betweenness_centrality(G_giant)\ntop_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\ndf_top_betweenness_centrality = pd.DataFrame(top_betweenness_centrality, columns=['Node', 'Betweenness Centrality'])\n\nprint(\"Top 10 nodes by betweenness centrality:\")\nprint(\"=======================================\")\nprint(df_top_betweenness_centrality)\n\ntop_nodes_by_ = df_top_betweenness_centrality['Node'].tolist()\nsubgraph = G_giant.subgraph(top_nodes_by_)\nnode_color = [betweenness_centrality[node] for node in subgraph.nodes()]\nnorm = mpl.colors.Normalize(vmin=min(node_color), vmax=max(node_color))\nnode_color_normalized = [norm(value) for value in node_color]\ncmap = plt.cm.cool\n\n# -------- #\n# Plotting #\n# -------- #\nplt.figure(figsize=(7, 3))\n\nnx.draw(subgraph, with_labels=True, node_size=300, edge_color='gray', font_size=8, \n        node_color=node_color_normalized, cmap=cmap)\nplt.title('Top 10 Betweenness Centrality Nodes')\nplt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), label='Betweenness Centrality')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:15.626735Z","iopub.execute_input":"2025-05-19T21:16:15.627004Z","iopub.status.idle":"2025-05-19T21:16:16.935167Z","shell.execute_reply.started":"2025-05-19T21:16:15.626977Z","shell.execute_reply":"2025-05-19T21:16:16.934287Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Closeness centrality.\ncloseness_centrality = nx.closeness_centrality(G_giant)\ntop_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\ndf_top_closeness_centrality = pd.DataFrame(top_closeness_centrality, columns=['Node', 'Closeness Centrality'])\n\nprint(\"Top 10 nodes by closeness centrality:\")\nprint(\"=====================================\")\nprint(df_top_closeness_centrality)\n\ntop_nodes_by_closeness = df_top_closeness_centrality['Node'].tolist()\nsubgraph = G_giant.subgraph(top_nodes_by_closeness)\nnode_color = [closeness_centrality[node] for node in subgraph.nodes()]\nnorm = mpl.colors.Normalize(vmin=min(node_color), vmax=max(node_color))\nnode_color_normalized = [norm(value) for value in node_color]\ncmap = plt.cm.cool\n\n# -------- #\n# Plotting #\n# -------- #\nplt.figure(figsize=(7, 3))\n\nnx.draw(subgraph, with_labels=True, node_size=300, edge_color='gray', font_size=8, \n        node_color=node_color_normalized, cmap=cmap)\nplt.title('Top 10 Closeness Centrality Nodes')\nplt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), label='Closeness Centrality')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:16.93627Z","iopub.execute_input":"2025-05-19T21:16:16.936553Z","iopub.status.idle":"2025-05-19T21:16:17.773417Z","shell.execute_reply.started":"2025-05-19T21:16:16.936525Z","shell.execute_reply":"2025-05-19T21:16:17.772441Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üñºÔ∏è Graph Visualization","metadata":{}},{"cell_type":"code","source":"# --------- #\n# Preparing #\n# --------- #\n\n# Identify nodes that are top in each centrality measure and their combinations.\ntop_nodes_degree = set([node for node, _ in top_degree_centrality])\ntop_nodes_betweenness = set([node for node, _ in top_betweenness_centrality])\ntop_nodes_closeness = set([node for node, _ in top_closeness_centrality])\n\ntop_nodes_both = top_nodes_degree.intersection(top_nodes_betweenness)\ntop_nodes_closeness_and_degree = top_nodes_closeness.intersection(top_nodes_degree)\ntop_nodes_closeness_and_betweenness = top_nodes_closeness.intersection(top_nodes_betweenness)\ntop_nodes_all_three = top_nodes_closeness.intersection(top_nodes_degree).intersection(top_nodes_betweenness)\n\n# Assign colors and shapes based on centrality measures.\nnode_color = []\nnode_shape = []\n\nfor node in G_giant.nodes():\n    if node in top_nodes_all_three:\n        node_color.append('purple')\n        node_shape.append('D')\n    elif node in top_nodes_closeness_and_degree:\n        node_color.append('orange')\n        node_shape.append('h')\n    elif node in top_nodes_closeness_and_betweenness:\n        node_color.append('cyan')\n        node_shape.append('v')\n    elif node in top_nodes_both:\n        node_color.append('green')\n        node_shape.append('s')\n    elif node in top_nodes_degree:\n        node_color.append('red')\n        node_shape.append('o')\n    elif node in top_nodes_betweenness:\n        node_color.append('magenta')\n        node_shape.append('^')\n    elif node in top_nodes_closeness:\n        node_color.append('yellow')\n        node_shape.append('h')\n    else:\n        node_color.append('slategrey')\n        node_shape.append('o')\n\n# -------- #\n# Plotting #\n# -------- #\n\nplt.figure(figsize=(15, 11))\n\npos = nx.spring_layout(G_giant)\n# Draw all nodes first.\nnx.draw_networkx_nodes(G_giant, pos, node_color='slategrey', node_size=10)\n\n# Draw nodes with specific centrality measures.\nnx.draw_networkx_nodes(G_giant, pos, nodelist=list(top_nodes_degree - top_nodes_both - top_nodes_closeness_and_degree), \n                       node_color='red', node_size=50, node_shape='o', \n                       label='Degree Centrality')\nnx.draw_networkx_nodes(G_giant, pos, nodelist=list(top_nodes_betweenness - top_nodes_both - top_nodes_closeness_and_betweenness), \n                       node_color='magenta', node_size=50, node_shape='^', \n                       label='Betweenness Centrality')\nnx.draw_networkx_nodes(G_giant, pos, nodelist=list(top_nodes_closeness - top_nodes_closeness_and_degree - top_nodes_closeness_and_betweenness), \n                       node_color='gold', node_size=50, node_shape='h', \n                       label='Closeness Centrality')\nnx.draw_networkx_nodes(G_giant, pos, nodelist=list(top_nodes_both), \n                       node_color='green', node_size=80, node_shape='s', \n                       label='Degree & Betweenness Centrality')\nnx.draw_networkx_nodes(G_giant, pos, nodelist=list(top_nodes_closeness_and_degree), \n                       node_color='orange', node_size=70, node_shape='h', \n                       label='Degree & Closeness Centrality')\nnx.draw_networkx_nodes(G_giant, pos, nodelist=list(top_nodes_closeness_and_betweenness), \n                       node_color='cyan', node_size=70, node_shape='v', \n                       label='Betweenness & Closeness Centrality')\nnx.draw_networkx_nodes(G_giant, pos, nodelist=list(top_nodes_all_three), \n                       node_color='purple', node_size=100, node_shape='D', \n                       label='Closeness, Degree, & Betweenness Centrality')\n\n# Draw edges.\nnx.draw_networkx_edges(G_giant, pos, width=0.8, edge_color='gray', alpha=0.5)\n\nplt.axis('off')\nplt.title('Nodes Highlighted by Degree, Betweenness, and Closeness Centrality')\nplt.legend(scatterpoints=1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:17.774818Z","iopub.execute_input":"2025-05-19T21:16:17.775102Z","iopub.status.idle":"2025-05-19T21:16:18.537633Z","shell.execute_reply.started":"2025-05-19T21:16:17.775075Z","shell.execute_reply":"2025-05-19T21:16:18.536678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìà Class-specific Analysis","metadata":{}},{"cell_type":"code","source":"# Class-based subgraphs.\nillicit_nodes = classes_sampled[classes_sampled['class_mapped'] == 'illicit']['txId']\nlicit_nodes = classes_sampled[classes_sampled['class_mapped'] == 'licit']['txId']\n\nG_illicit = G.subgraph(illicit_nodes)\nG_licit = G.subgraph(licit_nodes)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:18.538753Z","iopub.execute_input":"2025-05-19T21:16:18.539057Z","iopub.status.idle":"2025-05-19T21:16:18.548524Z","shell.execute_reply.started":"2025-05-19T21:16:18.539031Z","shell.execute_reply":"2025-05-19T21:16:18.547813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes_df","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:18.54981Z","iopub.execute_input":"2025-05-19T21:16:18.550693Z","iopub.status.idle":"2025-05-19T21:16:18.570927Z","shell.execute_reply.started":"2025-05-19T21:16:18.55053Z","shell.execute_reply":"2025-05-19T21:16:18.570008Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nnx.draw(G_illicit, with_labels=False, node_size=10, node_color='indianred', edge_color='black')\nplt.title('Illicit Transactions Subgraph')\n\nplt.subplot(1, 2, 2)\nnx.draw(G_licit, with_labels=False, node_size=10, node_color='forestgreen', edge_color='black')\nplt.title('Licit Transactions Subgraph')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:18.571964Z","iopub.execute_input":"2025-05-19T21:16:18.57225Z","iopub.status.idle":"2025-05-19T21:16:35.709922Z","shell.execute_reply.started":"2025-05-19T21:16:18.572223Z","shell.execute_reply":"2025-05-19T21:16:35.70899Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__‚ùå Illicit Transactions Graph__\n- Shows a fairly sparse distribution of nodes, with a ring-like pattern.\n- Most nodes are on the periphery, and a few nodes are concentrated in the center.\n- Suggests that the __illicit transactions may be less interconnected__, with a __few central nodes potentially acting as hubs__ or key points in the network.\n\n__‚úÖ Licit Transactions Graph__\n- The higher density of nodes suggests a more complex network with more transactions or interactions between entities.\n- Could imply a more established or legitimate network.\n- Suggests that licit transactions may involve more interconnected entities, with many nodes being closely related or interacting more frequently.","metadata":{}},{"cell_type":"markdown","source":"# üë• Community Detection\n\nThe goal of community detection is to identify clusters or communities within the graph where nodes are more densely connected internally than with the rest of the graph.","metadata":{}},{"cell_type":"code","source":"# Compute best partition.\npartition = community_louvain.best_partition(G_giant)\n\n# Draw graph.\nplt.figure(figsize=(10, 8))\n\npos = nx.spring_layout(G_giant)\ncmap = plt.get_cmap('viridis')\nnx.draw_networkx_nodes(G_giant, pos, node_color=list(partition.values()), node_size=8, cmap=cmap)\nnx.draw_networkx_edges(G_giant, pos, alpha=0.5, edge_color='grey')\nplt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:35.711314Z","iopub.execute_input":"2025-05-19T21:16:35.712077Z","iopub.status.idle":"2025-05-19T21:16:36.325789Z","shell.execute_reply.started":"2025-05-19T21:16:35.712025Z","shell.execute_reply":"2025-05-19T21:16:36.32485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß† Graph Neural Networks \n\nGraph Neural Networks (GNNs) are a deep learning technique specifically designed for graph data, useful for tasks like node classification, graph classification, or link prediction. In our case, for now, we will perform ___node prediction___.","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:36.326816Z","iopub.execute_input":"2025-05-19T21:16:36.327088Z","iopub.status.idle":"2025-05-19T21:16:36.33161Z","shell.execute_reply.started":"2025-05-19T21:16:36.327062Z","shell.execute_reply":"2025-05-19T21:16:36.330723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics_per_gnn = {\n    'gcn': {\n        'val': {\n            'precisions': [],\n            'probas': [],            \n        },\n        'test': {\n            'licit': {\n                'probas': []                \n            },\n            'illicit': {\n                'probas': []                \n            }, \n        }\n    },\n    'gat': {\n        'val': {\n            'precisions': [],\n            'probas': [],            \n        },\n        'test': {\n            'licit': {\n                'probas': []                \n            },\n            'illicit': {\n                'probas': []                \n            }, \n        }\n    },\n    'gin': {\n        'val': {\n            'precisions': [],\n            'probas': [],           \n        },\n        'test': {\n            'licit': {\n                'probas': []                \n            },\n            'illicit': {\n                'probas': []                \n            },            \n        }\n    }    \n}","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:36.332951Z","iopub.execute_input":"2025-05-19T21:16:36.333389Z","iopub.status.idle":"2025-05-19T21:16:36.34625Z","shell.execute_reply.started":"2025-05-19T21:16:36.333342Z","shell.execute_reply":"2025-05-19T21:16:36.345404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõ†Ô∏è Pre-Processing","metadata":{}},{"cell_type":"code","source":"num_edges = elliptic_txs_edgelist.shape[0]\nnum_nodes = elliptic_txs_features.shape[0]\n\nprint(f'Number of edges in the graph: {num_edges:8,}')\nprint(f'Number of nodes in the graph: {num_nodes:8,}')","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:36.347386Z","iopub.execute_input":"2025-05-19T21:16:36.347626Z","iopub.status.idle":"2025-05-19T21:16:36.358066Z","shell.execute_reply.started":"2025-05-19T21:16:36.347596Z","shell.execute_reply":"2025-05-19T21:16:36.357318Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------- #\n# Create mapping with txId as key and actual index as value #\n# --------------------------------------------------------- #\n\ntx_id_mapping = {tx_id: idx for idx, tx_id in enumerate(elliptic_txs_features['txId'])}\n\nedges_with_features = elliptic_txs_edgelist[elliptic_txs_edgelist['txId1'].isin(list(tx_id_mapping.keys()))\\\n                                          & elliptic_txs_edgelist['txId2'].isin(list(tx_id_mapping.keys()))]\n\nedges_with_features['Id1'] = edges_with_features['txId1'].map(tx_id_mapping)\nedges_with_features['Id2'] = edges_with_features['txId2'].map(tx_id_mapping)\n\nedges_with_features","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:36.359062Z","iopub.execute_input":"2025-05-19T21:16:36.359628Z","iopub.status.idle":"2025-05-19T21:16:36.700013Z","shell.execute_reply.started":"2025-05-19T21:16:36.359594Z","shell.execute_reply":"2025-05-19T21:16:36.69904Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"edge_index = torch.tensor(edges_with_features[['Id1', 'Id2']].values.T, dtype=torch.long)\nedge_index","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:36.701313Z","iopub.execute_input":"2025-05-19T21:16:36.701778Z","iopub.status.idle":"2025-05-19T21:16:36.767643Z","shell.execute_reply.started":"2025-05-19T21:16:36.701744Z","shell.execute_reply":"2025-05-19T21:16:36.766819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------- #\n# Save node features in suitable format #\n# ------------------------------------- #\n\nnode_features = torch.tensor(elliptic_txs_features.drop(columns=['txId']).values, \n                             dtype=torch.float)\nprint(node_features.shape)\nnode_features","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:36.768626Z","iopub.execute_input":"2025-05-19T21:16:36.768895Z","iopub.status.idle":"2025-05-19T21:16:37.112374Z","shell.execute_reply.started":"2025-05-19T21:16:36.768869Z","shell.execute_reply":"2025-05-19T21:16:37.111547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 1st dimension tells us about the `# of nodes`\n- 2nd dimension tells us about the `# of node features`","metadata":{}},{"cell_type":"code","source":"elliptic_txs_classes['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.1133Z","iopub.execute_input":"2025-05-19T21:16:37.113602Z","iopub.status.idle":"2025-05-19T21:16:37.13775Z","shell.execute_reply.started":"2025-05-19T21:16:37.11357Z","shell.execute_reply":"2025-05-19T21:16:37.136774Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------ #\n# Labelencode target class #\n# ------------------------ #\n\nle = LabelEncoder()\nclass_labels = le.fit_transform(elliptic_txs_classes['class'])\nnode_labels = torch.tensor(class_labels, dtype=torch.long)\noriginal_labels = le.inverse_transform(class_labels)\n\nprint(original_labels)\nprint(class_labels)\nprint(node_labels)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.139912Z","iopub.execute_input":"2025-05-19T21:16:37.140761Z","iopub.status.idle":"2025-05-19T21:16:37.198036Z","shell.execute_reply.started":"2025-05-19T21:16:37.140719Z","shell.execute_reply":"2025-05-19T21:16:37.197243Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(le.inverse_transform([0])) # illicit\nprint(le.inverse_transform([1])) # licit \nprint(le.inverse_transform([2])) # unknown","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.198901Z","iopub.execute_input":"2025-05-19T21:16:37.199272Z","iopub.status.idle":"2025-05-19T21:16:37.206419Z","shell.execute_reply.started":"2025-05-19T21:16:37.199233Z","shell.execute_reply":"2025-05-19T21:16:37.205691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------ #\n# Create pytorch geometric Data object #\n# ------------------------------------ #\n\ndata = Data(x=node_features, \n            edge_index=edge_index, \n            y=node_labels)\n\n# Move data to GPU.\ndata = data.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.207269Z","iopub.execute_input":"2025-05-19T21:16:37.207581Z","iopub.status.idle":"2025-05-19T21:16:37.45623Z","shell.execute_reply.started":"2025-05-19T21:16:37.207545Z","shell.execute_reply":"2025-05-19T21:16:37.455293Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we have created the [pytorch geometric Data object](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html). However, we still have lots of _unknown_ ü§∑ in this object which we dont want to train our GNN on. We only want to train on known nodes, thus, those which are either _licit_ ‚úÖ or _illicit_ ‚ùå. ","metadata":{}},{"cell_type":"code","source":"known_mask   = (data.y == 0) | (data.y == 1)  # Only nodes with known labels licit or illicit\nunknown_mask = data.y == 2                    # Nodes with unknown labels","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.457496Z","iopub.execute_input":"2025-05-19T21:16:37.457793Z","iopub.status.idle":"2025-05-19T21:16:37.510715Z","shell.execute_reply.started":"2025-05-19T21:16:37.457765Z","shell.execute_reply":"2025-05-19T21:16:37.509808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------ #\n# Define size for Training, Validation and Testing #\n# ------------------------------------------------ #\n\nnum_known_nodes = known_mask.sum().item()\npermutations = torch.randperm(num_known_nodes)\ntrain_size = int(0.8 * num_known_nodes)\nval_size = int(0.1 * num_known_nodes)\ntest_size = num_known_nodes - train_size - val_size\n\ntotal = np.sum([train_size, val_size, test_size])\n\nprint(f\"\"\"Number of observations per split\n    Training   : {train_size:10,} ({100*train_size/total:0.2f} %)\n    Validation : {val_size:10,} ({100*val_size/total:0.2f} %)\n    Testing    : {test_size:10,} ({100*test_size/total:0.2f} %)\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.511833Z","iopub.execute_input":"2025-05-19T21:16:37.512145Z","iopub.status.idle":"2025-05-19T21:16:37.572268Z","shell.execute_reply.started":"2025-05-19T21:16:37.512115Z","shell.execute_reply":"2025-05-19T21:16:37.571453Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------------------------- #\n# Create mask for the indices of Train, Val, Test #\n# ----------------------------------------------- #\n\ndata.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\ndata.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\ndata.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n\ntrain_indices = known_mask.nonzero(as_tuple=True)[0][permutations[:train_size]]\nval_indices = known_mask.nonzero(as_tuple=True)[0][permutations[train_size:train_size + val_size]]\ntest_indices = known_mask.nonzero(as_tuple=True)[0][permutations[train_size + val_size:]]\n\ndata.train_mask[train_indices] = True\ndata.val_mask[val_indices] = True\ndata.test_mask[test_indices] = True\n\ndata.train_mask","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.573183Z","iopub.execute_input":"2025-05-19T21:16:37.573841Z","iopub.status.idle":"2025-05-19T21:16:37.653491Z","shell.execute_reply.started":"2025-05-19T21:16:37.573806Z","shell.execute_reply":"2025-05-19T21:16:37.652618Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------- #\n# Statistics of the datasets #\n# -------------------------- #\n\ntrain_licit, train_illicit = (data.y[data.train_mask] == 1).sum().item(), (data.y[data.train_mask] == 0).sum().item()\nval_licit, val_illicit = (data.y[data.val_mask] == 1).sum().item(), (data.y[data.val_mask] == 0).sum().item()\ntest_licit, test_illicit = (data.y[data.test_mask] == 1).sum().item(), (data.y[data.test_mask] == 0).sum().item()\n\n# Calculate total counts.\ntrain_total = train_licit + train_illicit\nval_total = val_licit + val_illicit\ntest_total = test_licit + test_illicit\n\n# Calculate percentages.\ntrain_licit_pct = (train_licit / train_total) * 100\ntrain_illicit_pct = (train_illicit / train_total) * 100\nval_licit_pct = (val_licit / val_total) * 100\nval_illicit_pct = (val_illicit / val_total) * 100\ntest_licit_pct = (test_licit / test_total) * 100\ntest_illicit_pct = (test_illicit / test_total) * 100\n\npd.DataFrame({\n    'Set': ['Training', 'Validation', 'Testing'],\n    'Total Count': [train_total, val_total, test_total],\n    'Licit': [train_licit, val_licit, test_licit],\n    'Licit (%)': [train_licit_pct, val_licit_pct, test_licit_pct],\n    'Illicit': [train_illicit, val_illicit, test_illicit],\n    'Illicit (%)': [train_illicit_pct, val_illicit_pct, test_illicit_pct]\n})","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.654476Z","iopub.execute_input":"2025-05-19T21:16:37.655038Z","iopub.status.idle":"2025-05-19T21:16:37.680969Z","shell.execute_reply.started":"2025-05-19T21:16:37.654999Z","shell.execute_reply":"2025-05-19T21:16:37.679073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The percentages are pretty similar over all datasets, which is what we want. ","metadata":{}},{"cell_type":"code","source":"mapped_classes = np.array(['illicit', 'licit'])","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.682292Z","iopub.execute_input":"2025-05-19T21:16:37.682681Z","iopub.status.idle":"2025-05-19T21:16:37.686823Z","shell.execute_reply.started":"2025-05-19T21:16:37.682625Z","shell.execute_reply":"2025-05-19T21:16:37.686065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1Ô∏è‚É£ - GCN - Graph Convolutional Networks\n\nThe core idea of a _Graph Convolutional Networks_ (GCN) is to aggregate features from a node's local neighborhood. The feature aggregation is performed using a convolution operation adapted for graph structures.\n\n$$\nH^{(l+1)} = \\sigma\\left( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)} \\right)\n$$\n\n- $H^{(l)}$: Node feature matrix at layer $l$.\n- $\\tilde{A} = A + I$: Adjacency matrix with added self-loops.\n- $\\tilde{D}$: Diagonal degree matrix of $\\tilde{A}$.\n- $W^{(l)}$: Trainable weight matrix at layer $l$.\n- $\\sigma$: Activation function (e.g., ReLU).","metadata":{}},{"cell_type":"code","source":"# -------------- #\n# Define the GCN #\n# -------------- #\n\nclass GCN(torch.nn.Module):\n    def __init__(self, num_node_features, num_classes):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(num_node_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n\n\n# ---------- #\n# Initialize #\n# ---------- #\n\nmodel = GCN(num_node_features=data.num_features, num_classes=len(le.classes_)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), \n                             lr=0.01, \n                             weight_decay=0.0005)\ncriterion = torch.nn.CrossEntropyLoss()  # Since we have a multiclass classification problem.\n\ndata = data.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.687856Z","iopub.execute_input":"2025-05-19T21:16:37.689869Z","iopub.status.idle":"2025-05-19T21:16:37.790985Z","shell.execute_reply.started":"2025-05-19T21:16:37.689827Z","shell.execute_reply":"2025-05-19T21:16:37.790123Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- #\n# Train #\n# ----- #\n\ntrain_val_metrics = train_gnn(NUM_EPOCHS, \n                              data, \n                              model, \n                              optimizer, \n                              criterion)\n\nmetrics_per_gnn['gcn']['val']['precisions'] = train_val_metrics['val']['precisions']","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:37.79271Z","iopub.execute_input":"2025-05-19T21:16:37.793088Z","iopub.status.idle":"2025-05-19T21:16:42.430822Z","shell.execute_reply.started":"2025-05-19T21:16:37.793048Z","shell.execute_reply":"2025-05-19T21:16:42.430081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------- #\n# Evaluate #\n# -------- #\n\nmodel.eval()\nwith torch.no_grad():\n    \n    test_metrics = evaluate(model, data, data.test_mask)\n    test_acc = test_metrics.get('accuracy')\n    test_prec = test_metrics.get('precision')\n    test_rec = test_metrics.get('recall')\n    test_f1 = test_metrics.get('f1_score')\n\n    print(f'Test Acc: {test_acc:.4f} - Prec: {test_prec:.4f} - Rec: {test_rec:.4f} - F1: {test_f1:.4f}')","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:42.431806Z","iopub.execute_input":"2025-05-19T21:16:42.432077Z","iopub.status.idle":"2025-05-19T21:16:42.447561Z","shell.execute_reply.started":"2025-05-19T21:16:42.432049Z","shell.execute_reply":"2025-05-19T21:16:42.446832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_train_val_test_metrics(train_val_metrics, test_metrics, NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:42.448541Z","iopub.execute_input":"2025-05-19T21:16:42.448877Z","iopub.status.idle":"2025-05-19T21:16:43.32082Z","shell.execute_reply.started":"2025-05-19T21:16:42.448839Z","shell.execute_reply":"2025-05-19T21:16:43.319952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pred = predict(model, data)[data.train_mask]\ntest_pred = predict(model, data)[data.test_mask]","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:43.321927Z","iopub.execute_input":"2025-05-19T21:16:43.322268Z","iopub.status.idle":"2025-05-19T21:16:43.334638Z","shell.execute_reply.started":"2025-05-19T21:16:43.322225Z","shell.execute_reply":"2025-05-19T21:16:43.333963Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(le.inverse_transform([0])) # illicit\nprint(le.inverse_transform([1])) # licit \nprint(le.inverse_transform([2])) # unknown","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:43.335785Z","iopub.execute_input":"2025-05-19T21:16:43.336039Z","iopub.status.idle":"2025-05-19T21:16:43.342341Z","shell.execute_reply.started":"2025-05-19T21:16:43.336014Z","shell.execute_reply":"2025-05-19T21:16:43.341485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Classification report ---\nprint(\"Classification Report\")\nprint(\"=====================\\n\")\n\n# Train.\ny_true_train = data.y[data.train_mask].cpu().numpy()\ny_pred_train = train_pred.cpu().numpy()\n\nreport_train = classification_report(y_true_train, y_pred_train, target_names=mapped_classes)\n\nprint(f\"{4*' '}TRAIN\")\nprint(\"---------\")\nprint(report_train)\n\n# Test.\ny_true_test = data.y[data.test_mask].cpu().numpy()\ny_pred_test = test_pred.cpu().numpy()\n\nreport_test = classification_report(y_true_test, y_pred_test, target_names=mapped_classes)\n\nprint(f\"{4*' '}TEST\")\nprint(\"--------\")\nprint(report_test)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:43.343665Z","iopub.execute_input":"2025-05-19T21:16:43.343989Z","iopub.status.idle":"2025-05-19T21:16:43.378213Z","shell.execute_reply.started":"2025-05-19T21:16:43.34396Z","shell.execute_reply":"2025-05-19T21:16:43.377496Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mapped_classes = np.array(['illicit', 'licit'])\n\n# --- Confusion matrix ---\ncm = confusion_matrix(data.y[data.test_mask].cpu(), test_pred.cpu())\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Greens, \n            annot_kws={'size': 15}, \n            xticklabels=mapped_classes,\n            yticklabels=mapped_classes, \n            linecolor='black', linewidth=0.5,\n            ax=ax)\nplt.xlabel('Predicted Class')\nplt.ylabel('Actual Class')\n\n# Annotate each cell with the percentage of that row.\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        count = cm[i, j]\n        percentage = cm_normalized[i, j] * 100\n        text = f'\\n({percentage:.1f}%)'\n        color = 'white' if percentage > 95 else 'black'\n        ax.text(j + 0.5, i + 0.6, text,\n                ha='center', va='center', fontsize=10, color=color)\n\nplt.title('Confusion Matrix (Test set)\\nGraph Convolutional Network (GCN)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:43.379115Z","iopub.execute_input":"2025-05-19T21:16:43.379378Z","iopub.status.idle":"2025-05-19T21:16:43.64534Z","shell.execute_reply.started":"2025-05-19T21:16:43.379354Z","shell.execute_reply":"2025-05-19T21:16:43.644478Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- The model is very good at predicting `unknown` class. Simply because it's the majority of the dataset.\n- The GCN is also decent at predicting if a node is `licit` or not. However, it predicted 42% of all `licit` nodes as being `unknown`. \n- The model is decent but unsure at predicting `illicit` nodes. It only classified 46% of all `illicit` nodes correctly.","metadata":{}},{"cell_type":"code","source":"train_probas = predict_probabilities(model, data)[data.train_mask]\ntest_probas = predict_probabilities(model, data)[data.test_mask]","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:43.646263Z","iopub.execute_input":"2025-05-19T21:16:43.646511Z","iopub.status.idle":"2025-05-19T21:16:43.658768Z","shell.execute_reply.started":"2025-05-19T21:16:43.646486Z","shell.execute_reply":"2025-05-19T21:16:43.658143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_probas_illicit = train_probas[:, 1].cpu().numpy()\ntrain_probas_illicit","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:43.659698Z","iopub.execute_input":"2025-05-19T21:16:43.659985Z","iopub.status.idle":"2025-05-19T21:16:43.666201Z","shell.execute_reply.started":"2025-05-19T21:16:43.659939Z","shell.execute_reply":"2025-05-19T21:16:43.665247Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ALPHA = 0.1\n\ntrain_probas_licit = train_probas[:, 0].cpu().numpy()\ntrain_probas_illicit = train_probas[:, 1].cpu().numpy()\n\ntest_probas_licit = test_probas[:, 0].cpu().numpy()\ntest_probas_illicit = test_probas[:, 1].cpu().numpy()\n\nmetrics_per_gnn['gcn']['test']['licit']['probas'] = test_probas_licit\nmetrics_per_gnn['gcn']['test']['illicit']['probas'] = test_probas_illicit\n\n# -------- #\n# Plotting #\n# -------- #\n\nplt.figure(figsize=(7, 4))\n\n# Plot licit class probabilities\nsns.kdeplot(train_probas_licit, fill=True, color=\"forestgreen\", linestyle='-', alpha=ALPHA, label=\"Training - Licit\")\nsns.kdeplot(test_probas_licit, fill=True, color=\"green\", linestyle='--', alpha=ALPHA, label=\"Test - Licit\")\n\n# Plot illicit class probabilities\nsns.kdeplot(train_probas_illicit, fill=True, color=\"salmon\", linestyle='-', alpha=ALPHA, label=\"Training - Illicit\")\nsns.kdeplot(test_probas_illicit, fill=True, color=\"red\", linestyle='--', alpha=ALPHA, label=\"Test - Illicit\")\n\nplt.title(\"GCN - Predicted Probabilities\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Density\")\nplt.legend(loc=\"upper center\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:43.667162Z","iopub.execute_input":"2025-05-19T21:16:43.667415Z","iopub.status.idle":"2025-05-19T21:16:44.56323Z","shell.execute_reply.started":"2025-05-19T21:16:43.667392Z","shell.execute_reply":"2025-05-19T21:16:44.562104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2Ô∏è‚É£ - GAT - Graph Attention Networks\n\n_Graph Attention Networks_ (GAT) introduce attention mechanisms to graphs, where the importance of neighboring nodes is learned via attention coefficients.\n\n$$\nh_i^{(l+1)} = \\sigma\\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(l)} W^{(l)} h_j^{(l)} \\right)\n$$\n\n- $h_i^{(l)}$: Feature vector of node $i$ at layer $l$.\n- $\\mathcal{N}(i)$: Set of neighbors of node $i$.\n- $\\alpha_{ij}^{(l)}$: Attention coefficient between node $i$ and node $j$.\n- $W^{(l)}$: Trainable weight matrix.\n- $\\sigma$: Activation function.\n\nThe attention coefficients $\\alpha_{ij}^{(l)}$ are computed as:\n\n$$\n\\alpha_{ij}^{(l)} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^T \\left[W^{(l)} h_i^{(l)} \\, \\| \\, W^{(l)} h_j^{(l)}\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^T \\left[W^{(l)} h_i^{(l)} \\, \\| \\, W^{(l)} h_k^{(l)}\\right]\\right)\\right)}\n$$\n\n- $\\mathbf{a}$: Attention mechanism parameter vector.\n- $\\|$: Concatenation operation.","metadata":{}},{"cell_type":"code","source":"# -------------- #\n# Define the GAT #\n# -------------- #\n\nclass GAT(torch.nn.Module):\n    def __init__(self, num_node_features, num_classes, num_heads=8):\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(num_node_features, 8, heads=num_heads, dropout=0.6)\n        self.conv2 = GATConv(8 * num_heads, num_classes, heads=1, concat=False, dropout=0.6)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n\n# ---------- #\n# Initialize #\n# ---------- #\n\nmodel = GAT(num_node_features=data.num_features, num_classes=len(le.classes_)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\ncriterion = torch.nn.CrossEntropyLoss()  # Since we have a multiclass classification problem.\n\ndata = data.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:44.564391Z","iopub.execute_input":"2025-05-19T21:16:44.564672Z","iopub.status.idle":"2025-05-19T21:16:45.058366Z","shell.execute_reply.started":"2025-05-19T21:16:44.564645Z","shell.execute_reply":"2025-05-19T21:16:45.05741Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- #\n# Train #\n# ----- #\n\ntrain_val_metrics = train_gnn(NUM_EPOCHS, \n                              data, \n                              model, \n                              optimizer, \n                              criterion)\n\nmetrics_per_gnn['gat']['val']['precisions'] = train_val_metrics['val']['precisions']","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:45.059527Z","iopub.execute_input":"2025-05-19T21:16:45.059875Z","iopub.status.idle":"2025-05-19T21:16:51.356635Z","shell.execute_reply.started":"2025-05-19T21:16:45.059838Z","shell.execute_reply":"2025-05-19T21:16:51.355766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------- #\n# Evaluate #\n# -------- #\n\nmodel.eval()\nwith torch.no_grad():\n    \n    test_metrics = evaluate(model, data, data.test_mask)\n    test_acc = test_metrics.get('accuracy')\n    test_prec = test_metrics.get('precision')\n    test_rec = test_metrics.get('recall')\n    test_f1 = test_metrics.get('f1_score')\n\n    print(f'Test Acc: {test_acc:.4f} - Prec: {test_prec:.4f} - Rec: {test_rec:.4f} - F1: {test_f1:.4f}')","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:51.357707Z","iopub.execute_input":"2025-05-19T21:16:51.357978Z","iopub.status.idle":"2025-05-19T21:16:51.378142Z","shell.execute_reply.started":"2025-05-19T21:16:51.357952Z","shell.execute_reply":"2025-05-19T21:16:51.377367Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_train_val_test_metrics(train_val_metrics, test_metrics, NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:51.379263Z","iopub.execute_input":"2025-05-19T21:16:51.379593Z","iopub.status.idle":"2025-05-19T21:16:52.296797Z","shell.execute_reply.started":"2025-05-19T21:16:51.379556Z","shell.execute_reply":"2025-05-19T21:16:52.295926Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pred = predict(model, data)[data.test_mask]\n\n# --- Confusion matrix ---\ncm = confusion_matrix(data.y[data.test_mask].cpu(), test_pred.cpu())\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Greens, \n            annot_kws={'size': 15}, \n            xticklabels=mapped_classes,\n            yticklabels=mapped_classes, \n            linecolor='black', linewidth=0.5,\n            ax=ax)\nplt.xlabel('Predicted Class')\nplt.ylabel('Actual Class')\n\n# Annotate each cell with the percentage of that row.\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        count = cm[i, j]\n        percentage = cm_normalized[i, j] * 100\n        text = f'\\n({percentage:.1f}%)'\n        color = 'white' if percentage > 95 else 'black'\n        ax.text(j + 0.5, i + 0.6, text,\n                ha='center', va='center', fontsize=10, color=color)\n\nplt.title('Confusion Matrix\\nGraph Attention Network (GAT)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:52.298096Z","iopub.execute_input":"2025-05-19T21:16:52.29882Z","iopub.status.idle":"2025-05-19T21:16:52.56897Z","shell.execute_reply.started":"2025-05-19T21:16:52.29878Z","shell.execute_reply":"2025-05-19T21:16:52.568198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_probas = predict_probabilities(model, data)[data.train_mask]\ntest_probas = predict_probabilities(model, data)[data.test_mask]","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:52.570168Z","iopub.execute_input":"2025-05-19T21:16:52.570541Z","iopub.status.idle":"2025-05-19T21:16:52.596693Z","shell.execute_reply.started":"2025-05-19T21:16:52.570503Z","shell.execute_reply":"2025-05-19T21:16:52.595923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_probas_illicit = train_probas[:, 1].cpu().numpy()\ntrain_probas_illicit","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:52.597664Z","iopub.execute_input":"2025-05-19T21:16:52.597904Z","iopub.status.idle":"2025-05-19T21:16:52.605427Z","shell.execute_reply.started":"2025-05-19T21:16:52.59788Z","shell.execute_reply":"2025-05-19T21:16:52.604572Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ALPHA = 0.1\n\ntrain_probas_licit = train_probas[:, 0].cpu().numpy()\ntrain_probas_illicit = train_probas[:, 1].cpu().numpy()\n\ntest_probas_licit = test_probas[:, 0].cpu().numpy()\ntest_probas_illicit = test_probas[:, 1].cpu().numpy()\n\nmetrics_per_gnn['gat']['test']['licit']['probas'] = test_probas_licit\nmetrics_per_gnn['gat']['test']['illicit']['probas'] = test_probas_illicit\n\n# -------- #\n# Plotting #\n# -------- #\n\nplt.figure(figsize=(7, 4))\n\n# Plot licit class probabilities\nsns.kdeplot(train_probas_licit, fill=True, color=\"forestgreen\", linestyle='-', alpha=ALPHA, label=\"Training - Licit\")\nsns.kdeplot(test_probas_licit, fill=True, color=\"green\", linestyle='--', alpha=ALPHA, label=\"Test - Licit\")\n\n# Plot illicit class probabilities\nsns.kdeplot(train_probas_illicit, fill=True, color=\"salmon\", linestyle='-', alpha=ALPHA, label=\"Training - Illicit\")\nsns.kdeplot(test_probas_illicit, fill=True, color=\"red\", linestyle='--', alpha=ALPHA, label=\"Test - Illicit\")\n\nplt.title(\"GAT - Predicted Probabilities\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Density\")\nplt.legend(loc=\"upper center\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:52.60653Z","iopub.execute_input":"2025-05-19T21:16:52.606817Z","iopub.status.idle":"2025-05-19T21:16:53.379966Z","shell.execute_reply.started":"2025-05-19T21:16:52.606775Z","shell.execute_reply":"2025-05-19T21:16:53.379037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3Ô∏è‚É£ - GIN - Graph Isomorphism Networks\n\n_Graph Isomorphism Networks_ (GINs) are designed to be as powerful as the Weisfeiler-Lehman graph isomorphism test, focusing on distinguishing graph structures.\n\n$$\nh_i^{(l+1)} = \\text{MLP}^{(l)}\\left( (1 + \\epsilon) \\cdot h_i^{(l)} + \\sum_{j \\in \\mathcal{N}(i)} h_j^{(l)} \\right)\n$$\n\n- $h_i^{(l)}$: Feature vector of node $i$ at layer $l$.\n- $\\mathcal{N}(i)$: Set of neighbors of node $i$.\n- $\\epsilon$: Learnable or fixed scalar.\n- $\\text{MLP}^{(l)}$: Multi-layer perceptron at layer $l$.\n\nGIN aggregates neighboring node features and then applies an MLP to produce the next layer's node features. The inclusion of $\\epsilon$ allows for distinguishing between a node and its neighbors.","metadata":{}},{"cell_type":"code","source":"# -------------- #\n# Define the GIN #\n# -------------- #7\n\nfrom torch_geometric.nn import GINConv, global_add_pool\nimport torch.nn.functional as F\n\nclass GIN(torch.nn.Module):\n    def __init__(self, num_node_features, num_classes):\n        super(GIN, self).__init__()\n        \n        # 1st GIN layer.\n        nn1 = torch.nn.Sequential(\n            torch.nn.Linear(num_node_features, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 64)\n        )\n        self.conv1 = GINConv(nn1)\n        \n        # 2nd GIN layer.\n        nn2 = torch.nn.Sequential(\n            torch.nn.Linear(64, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 64)\n        )\n        self.conv2 = GINConv(nn2)\n        self.fc = torch.nn.Linear(64, num_classes)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n#         x = global_add_pool(x, batch)\n        x = self.fc(x)\n        \n        return F.log_softmax(x, dim=1)\n\n# ---------- #\n# Initialize #\n# ---------- #\n\nmodel = GIN(num_node_features=data.num_features, num_classes=len(le.classes_)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# # Handle the class imbalance\n# class_counts = torch.bincount(data.y)\n# class_weights = 1. / class_counts.float()\n# criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n\ndata = data.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:53.381348Z","iopub.execute_input":"2025-05-19T21:16:53.381953Z","iopub.status.idle":"2025-05-19T21:16:53.434059Z","shell.execute_reply.started":"2025-05-19T21:16:53.381909Z","shell.execute_reply":"2025-05-19T21:16:53.433355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- #\n# Train #\n# ----- #\n\ntrain_val_metrics = train_gnn(NUM_EPOCHS, \n                              data, \n                              model, \n                              optimizer, \n                              criterion)\n\nmetrics_per_gnn['gin']['val']['precisions'] = train_val_metrics['val']['precisions']","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:16:53.435062Z","iopub.execute_input":"2025-05-19T21:16:53.435351Z","iopub.status.idle":"2025-05-19T21:17:01.919483Z","shell.execute_reply.started":"2025-05-19T21:16:53.435322Z","shell.execute_reply":"2025-05-19T21:17:01.918605Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------- #\n# Evaluate #\n# -------- #\n\nmodel.eval()\nwith torch.no_grad():\n    \n    test_metrics = evaluate(model, data, data.test_mask)\n    test_acc = test_metrics.get('accuracy')\n    test_prec = test_metrics.get('precision')\n    test_rec = test_metrics.get('recall')\n    test_f1 = test_metrics.get('f1_score')\n\n    print(f'Test Acc: {test_acc:.4f} - Prec: {test_prec:.4f} - Rec: {test_rec:.4f} - F1: {test_f1:.4f}')","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:01.927603Z","iopub.execute_input":"2025-05-19T21:17:01.927917Z","iopub.status.idle":"2025-05-19T21:17:01.967477Z","shell.execute_reply.started":"2025-05-19T21:17:01.927888Z","shell.execute_reply":"2025-05-19T21:17:01.966691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_train_val_test_metrics(train_val_metrics, test_metrics, NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:01.968395Z","iopub.execute_input":"2025-05-19T21:17:01.968634Z","iopub.status.idle":"2025-05-19T21:17:02.853414Z","shell.execute_reply.started":"2025-05-19T21:17:01.968604Z","shell.execute_reply":"2025-05-19T21:17:02.85245Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pred = predict(model, data)[data.test_mask]\n\n# --- Confusion matrix ---\ncm = confusion_matrix(data.y[data.test_mask].cpu(), test_pred.cpu())\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Greens, \n            annot_kws={'size': 15}, \n            xticklabels=mapped_classes,\n            yticklabels=mapped_classes, \n            linecolor='black', linewidth=0.5,\n            ax=ax)\nplt.xlabel('Predicted Class')\nplt.ylabel('Actual Class')\n\n# Annotate each cell with the percentage of that row.\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        count = cm[i, j]\n        percentage = cm_normalized[i, j] * 100\n        text = f'\\n({percentage:.1f}%)'\n        color = 'white' if percentage > 95 else 'black'\n        ax.text(j + 0.5, i + 0.6, text,\n                ha='center', va='center', fontsize=10, color=color)\n\nplt.title('Confusion Matrix\\nGraph Isomorphism Network (GIN)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:02.854922Z","iopub.execute_input":"2025-05-19T21:17:02.855344Z","iopub.status.idle":"2025-05-19T21:17:03.160292Z","shell.execute_reply.started":"2025-05-19T21:17:02.855305Z","shell.execute_reply":"2025-05-19T21:17:03.159198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_probas = predict_probabilities(model, data)[data.train_mask]\ntest_probas = predict_probabilities(model, data)[data.test_mask]","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:03.161455Z","iopub.execute_input":"2025-05-19T21:17:03.161763Z","iopub.status.idle":"2025-05-19T21:17:03.223054Z","shell.execute_reply.started":"2025-05-19T21:17:03.161732Z","shell.execute_reply":"2025-05-19T21:17:03.22209Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_probas_illicit = train_probas[:, 1].cpu().numpy()\ntrain_probas_illicit","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:03.224333Z","iopub.execute_input":"2025-05-19T21:17:03.224644Z","iopub.status.idle":"2025-05-19T21:17:03.231632Z","shell.execute_reply.started":"2025-05-19T21:17:03.224615Z","shell.execute_reply":"2025-05-19T21:17:03.230547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ALPHA = 0.1\n\ntrain_probas_licit = train_probas[:, 0].cpu().numpy()\ntrain_probas_illicit = train_probas[:, 1].cpu().numpy()\n\ntest_probas_licit = test_probas[:, 0].cpu().numpy()\ntest_probas_illicit = test_probas[:, 1].cpu().numpy()\n\nmetrics_per_gnn['gin']['test']['licit']['probas'] = test_probas_licit\nmetrics_per_gnn['gin']['test']['illicit']['probas'] = test_probas_illicit\n\n# -------- #\n# Plotting #\n# -------- #\n\nplt.figure(figsize=(7, 4))\n\n# Plot licit class probabilities\nsns.kdeplot(train_probas_licit, fill=True, color=\"forestgreen\", linestyle='-', alpha=ALPHA, label=\"Training - Licit\")\nsns.kdeplot(test_probas_licit, fill=True, color=\"green\", linestyle='--', alpha=ALPHA, label=\"Test - Licit\")\n\n# Plot illicit class probabilities\nsns.kdeplot(train_probas_illicit, fill=True, color=\"salmon\", linestyle='-', alpha=ALPHA, label=\"Training - Illicit\")\nsns.kdeplot(test_probas_illicit, fill=True, color=\"red\", linestyle='--', alpha=ALPHA, label=\"Test - Illicit\")\n\nplt.title(\"GIN - Predicted Probabilities\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Density\")\nplt.legend(loc=\"upper center\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:03.232884Z","iopub.execute_input":"2025-05-19T21:17:03.233292Z","iopub.status.idle":"2025-05-19T21:17:03.960487Z","shell.execute_reply.started":"2025-05-19T21:17:03.233262Z","shell.execute_reply":"2025-05-19T21:17:03.959636Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### T-SNE","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom cuml.manifold import TSNE as cuTSNE\nfrom sklearn.decomposition import PCA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T21:17:03.961548Z","iopub.execute_input":"2025-05-19T21:17:03.962422Z","iopub.status.idle":"2025-05-19T21:17:08.826279Z","shell.execute_reply.started":"2025-05-19T21:17:03.962393Z","shell.execute_reply":"2025-05-19T21:17:08.825253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Assume embeddings is your high-dimensional data\n# First reduce dimensionality with PCA to 2 components if you have only 3 features\npca = PCA(n_components=2, random_state=RANDOM_STATE)  # Reduce to 2D if only 3 features\nembeddings_pca = pca.fit_transform(embeddings)\n\n# Then apply t-SNE to the PCA-reduced data\ntsne = TSNE(n_components=2, random_state=RANDOM_STATE)\nembeddings_2d = tsne.fit_transform(embeddings_pca)\n\n# Visualize the t-SNE results, colored by class labels\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=data.y.cpu().numpy(), cmap=\"jet\", alpha=0.7)\nplt.colorbar(scatter, label='Class label')\nplt.title(\"t-SNE visualization of GIN node embeddings\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:08.827889Z","iopub.execute_input":"2025-05-19T21:17:08.828927Z","iopub.status.idle":"2025-05-19T21:17:09.320749Z","shell.execute_reply.started":"2025-05-19T21:17:08.82887Z","shell.execute_reply":"2025-05-19T21:17:09.319385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------ #\n# Randomly subsample nodes #\n# ------------------------ #\n\nsubset_size = 5_000 \nindices = np.random.choice(len(embeddings), subset_size, replace=False)\nembeddings_subset = embeddings[indices]\nlabels_subset = data.y.cpu().numpy()[indices]\n\ntsne = TSNE(n_components=2, random_state=RANDOM_STATE)\nembeddings_2d = tsne.fit_transform(embeddings_subset)\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=2, c=labels_subset, cmap=\"jet\", alpha=0.7)\nplt.colorbar(scatter, label='Class label')\nplt.title(\"t-SNE visualization of GIN node embeddings (subsampled)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:09.321536Z","iopub.status.idle":"2025-05-19T21:17:09.32188Z","shell.execute_reply.started":"2025-05-19T21:17:09.321725Z","shell.execute_reply":"2025-05-19T21:17:09.321742Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------- #\n# GPU-Accelerated t-SNE #\n# --------------------- #\n\ntsne = cuTSNE(n_components=2, random_state=RANDOM_STATE)#perplexity=10, \nembeddings_2d = tsne.fit_transform(embeddings)\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=2, c=data.y.cpu().numpy(), cmap=\"jet\", alpha=0.7)\nplt.colorbar(scatter, label='Class label')\nplt.title(\"GPU-accelerated t-SNE visualization of GIN node embeddings\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:09.323405Z","iopub.status.idle":"2025-05-19T21:17:09.323721Z","shell.execute_reply.started":"2025-05-19T21:17:09.323576Z","shell.execute_reply":"2025-05-19T21:17:09.323591Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Graph Explainability - `GNNExplainer` - TBD\n\nThe [GNN-Explainer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.explain.algorithm.GNNExplainer.html#torch_geometric.explain.algorithm.GNNExplainer) model from the paper [\"GNNExplainer: Generating Explanations for Graph Neural Networks\"](https://arxiv.org/abs/1903.03894) can be used to identify compact subgraph structures and node features that play a crucial role the predictions made by the GNN.","metadata":{}},{"cell_type":"markdown","source":"---\n\n# ‚öñÔ∏è Compare the GNNs ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\n\nepochs = range(1, len(metrics_per_gnn['gcn']['val']['precisions']) + 1)\n\nplt.plot(epochs, metrics_per_gnn['gcn']['val']['precisions'], color='C0', linewidth=1.2, linestyle='--', label='GCN')\nplt.plot(epochs, metrics_per_gnn['gat']['val']['precisions'], color='C1', linewidth=1.2, linestyle='-.', label='GAT')\nplt.plot(epochs, metrics_per_gnn['gin']['val']['precisions'], color='C2', linewidth=1.2, linestyle=':', label='GIN')\n\nplt.xlabel('Epoch')\nplt.ylabel('Precision')\nplt.title('Validation Precisions Comparison Across GNN Models')\nplt.legend(fontsize=10)\nplt.grid(False)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:09.324641Z","iopub.status.idle":"2025-05-19T21:17:09.325002Z","shell.execute_reply.started":"2025-05-19T21:17:09.324827Z","shell.execute_reply":"2025-05-19T21:17:09.324846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Conclusion:__\n\n> GIN seems to perform the best overall in terms of precision, especially after stabilizing in the later epochs. It achieved the highest precision but showed initial instability.","metadata":{}},{"cell_type":"code","source":"ALPHA = 0.9\nFILL = False\n\nplt.figure(figsize=(10, 6))\n\nsns.kdeplot(metrics_per_gnn['gcn']['test']['licit']['probas'], fill=FILL, color=\"forestgreen\", linestyle='-', alpha=ALPHA, label=\"GCN - Licit\")\nsns.kdeplot(metrics_per_gnn['gcn']['test']['illicit']['probas'], fill=FILL, color=\"salmon\", linestyle='-', alpha=ALPHA, label=\"GCN - Illicit\")\n\nsns.kdeplot(metrics_per_gnn['gat']['test']['licit']['probas'], fill=FILL, color=\"darkgreen\", linestyle='--', alpha=ALPHA, label=\"GAT - Licit\")\nsns.kdeplot(metrics_per_gnn['gat']['test']['illicit']['probas'], fill=FILL, color=\"red\", linestyle='--', alpha=ALPHA, label=\"GAT - Illicit\")\n\nsns.kdeplot(metrics_per_gnn['gin']['test']['licit']['probas'], fill=FILL, color=\"lightgreen\", linestyle=':', alpha=ALPHA, label=\"GIN - Licit\")\nsns.kdeplot(metrics_per_gnn['gin']['test']['illicit']['probas'], fill=FILL, color=\"darkred\", linestyle=':', alpha=ALPHA, label=\"GIN - Illicit\")\n\nplt.title(\"Predicted Probabilities Comparison Across GNN Models\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Density\")\nplt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.1), ncol=2)\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:09.326329Z","iopub.status.idle":"2025-05-19T21:17:09.326678Z","shell.execute_reply.started":"2025-05-19T21:17:09.326489Z","shell.execute_reply":"2025-05-19T21:17:09.326504Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_temp = []\n\nfor model in ['gcn', 'gat', 'gin']:\n    for category in ['licit', 'illicit']:\n        probas = metrics_per_gnn[model]['test'][category]['probas']\n        data_temp.extend([(model.upper(), category.capitalize(), proba) for proba in probas])\n\ntemp = pd.DataFrame(data_temp, columns=['Model', 'Class', 'Probability'])\n\nplt.figure(figsize=(8, 4))\n\nflierprops = dict(marker='o', markerfacecolor='None', markersize=5,  markeredgecolor='C0', alpha=0.2)\n\nax = sns.boxplot(y='Model', x='Probability', hue='Class', data=temp, \n                 linewidth=2.5, \n                 fliersize=0.5,\n                 palette={\n                     'Licit': 'forestgreen', \n                     'Illicit': 'indianred'\n                 },\n                 flierprops=flierprops\n)\n\n    \nplt.grid(True, which='both', axis='x', color='lightgrey', linestyle='-', linewidth=0.5)\nplt.title(\"Comparison of Predicted Probabilities Across GNN's\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"GNN Model\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5), ncol=1)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:09.327996Z","iopub.status.idle":"2025-05-19T21:17:09.328325Z","shell.execute_reply.started":"2025-05-19T21:17:09.328152Z","shell.execute_reply":"2025-05-19T21:17:09.328167Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------- #\n# Running a t-test to compare the means of    #\n# licit vs illicit predictions for each model #\n# ------------------------------------------- #\n\ncomparisons = [('gcn', 'gat'), ('gcn', 'gin'), ('gat', 'gin')]\nresults = []\n\nfor model1, model2 in comparisons:\n    for category in ['licit', 'illicit']:\n        probas1 = metrics_per_gnn[model1]['test'][category]['probas']\n        probas2 = metrics_per_gnn[model2]['test'][category]['probas']\n        t_stat, p_val = ttest_ind(probas1, probas2, equal_var=False)\n        if p_val < 0.05:\n            results.append(f\"Significant difference between {model1.upper()} and {model2.upper()} for {category.capitalize()} predictions (p-value: {p_val:.4f})\")\n\nfor result in results:\n    print(result)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:09.329628Z","iopub.status.idle":"2025-05-19T21:17:09.329932Z","shell.execute_reply.started":"2025-05-19T21:17:09.329789Z","shell.execute_reply":"2025-05-19T21:17:09.329804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combinations = [('GCN', 'GAT'), ('GCN', 'GIN'), ('GAT', 'GIN')]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nfor ax, (model1, model2) in zip(axes, combinations):\n    scatter_data = pd.DataFrame({\n        f'{model1}_Probabilities': temp[temp['Model'] == model1]['Probability'].values,\n        f'{model2}_Probabilities': temp[temp['Model'] == model2]['Probability'].values\n    })    \n    \n    sns.scatterplot(\n        data=scatter_data,\n        x=f'{model1}_Probabilities',\n        y=f'{model2}_Probabilities',\n        color='C0',\n        edgecolor='white',\n        marker='o',        \n        alpha=0.3, ax=ax\n    )\n    \n    corr_value = np.corrcoef(temp[temp['Model'] == model1]['Probability'],\n                             temp[temp['Model'] == model2]['Probability'])[0, 1]\n    \n    ax.plot([0, 1], [0, 1], ls='--', color='lightcoral', label=f'Perfect Correlation', alpha=1.0)\n    \n    ax.set_title(f'{model1} vs {model2}\\nCorr: {corr_value:.3f}')\n    ax.set_xlabel(f'{model1} Probabilities')\n    ax.set_ylabel(f'{model2} Probabilities')\n    ax.legend(loc='upper left')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-19T21:17:09.331449Z","iopub.status.idle":"2025-05-19T21:17:09.331752Z","shell.execute_reply.started":"2025-05-19T21:17:09.331607Z","shell.execute_reply":"2025-05-19T21:17:09.331623Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ToDos\n\n- [x] Outsource the code used for each GNN into global methods. Will make the notebook shorter and more readable.\n- [x] Train only on known nodes. Then validate on a small set of known nodes which was not trained on. Then predict on the unknown.\n- [x] Implement GIN as well. \n- [x] Compare all the models. \n- [ ] Add short explanations for each GNN type + formulas. \n- [ ] Develop a baseline model (non-GNN). \n- [ ] Perform some graph explanability methods. for example GNNExplainer. \n- [ ] Perform analysis on different sizes of node features and how they influence the performance.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\nüíö Thank you for reading üíö\n\nIf you have any questions or feedback, feel free to leave a comment ü§î\n\nThis notebook is __still in progress__.\n\nPlease __UPVOTE__ if you enjoyed this notebook üôè","metadata":{}},{"cell_type":"markdown","source":"# Community Detection Algorithms","metadata":{}},{"cell_type":"markdown","source":"## Louvain Method (Modularity Maximisation)","metadata":{}},{"cell_type":"code","source":"# Louvain Method (Modularity Maximisation)\n# Already demonstrated above, but here's a concise example for reference:\nimport networkx as nx\nimport community as community_louvain\n\n# Use the giant component for community detection\npartition = community_louvain.best_partition(G_giant)\nnum_communities = len(set(partition.values()))\nprint(f\"Number of communities detected by Louvain: {num_communities}\")\n\n# Optionally, visualize communities\nplt.figure(figsize=(8, 6))\npos = nx.spring_layout(G_giant, seed=RANDOM_STATE)\nnx.draw_networkx_nodes(G_giant, pos, node_color=list(partition.values()), cmap=plt.cm.viridis, node_size=10)\nnx.draw_networkx_edges(G_giant, pos, alpha=0.3)\nplt.title(\"Communities detected by Louvain Method\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T21:17:29.928616Z","iopub.execute_input":"2025-05-19T21:17:29.929222Z","iopub.status.idle":"2025-05-19T21:17:30.581664Z","shell.execute_reply.started":"2025-05-19T21:17:29.929151Z","shell.execute_reply":"2025-05-19T21:17:30.58072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Label Propagation Algorithm (LPA)","metadata":{}},{"cell_type":"code","source":"# Label Propagation Algorithm (LPA)\n# Detect communities using label propagation\nlpa_communities = nx.algorithms.community.label_propagation_communities(G_giant)\nlpa_communities = list(lpa_communities)\nprint(f\"Number of communities detected by Label Propagation: {len(lpa_communities)}\")\n\n# Assign community labels to nodes for visualization\nlpa_partition = {}\nfor i, comm in enumerate(lpa_communities):\n    for node in comm:\n        lpa_partition[node] = i\n\nplt.figure(figsize=(8, 6))\nnx.draw_networkx_nodes(G_giant, pos, node_color=[lpa_partition[n] for n in G_giant.nodes()], cmap=plt.cm.tab20, node_size=10)\nnx.draw_networkx_edges(G_giant, pos, alpha=0.3)\nplt.title(\"Communities detected by Label Propagation\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T21:17:35.064429Z","iopub.execute_input":"2025-05-19T21:17:35.064775Z","iopub.status.idle":"2025-05-19T21:17:35.305129Z","shell.execute_reply.started":"2025-05-19T21:17:35.064745Z","shell.execute_reply":"2025-05-19T21:17:35.304112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Node2Vec","metadata":{}},{"cell_type":"code","source":"# Node2Vec Embedding & Community Detection\nfrom node2vec import Node2Vec\n\n# Fit node2vec model on the giant component\nnode2vec = Node2Vec(G_giant, dimensions=32, walk_length=20, num_walks=100, workers=2, seed=RANDOM_STATE)\nn2v_model = node2vec.fit(window=10, min_count=1, batch_words=4)\n\n# Get embeddings for each node\nembeddings = np.array([n2v_model.wv[str(node)] for node in G_giant.nodes()])\nprint(f\"Node2Vec embeddings shape: {embeddings.shape}\")\n\n# Optionally, visualize embeddings using t-SNE\nfrom sklearn.manifold import TSNE\nembeddings_2d = TSNE(n_components=2, random_state=RANDOM_STATE).fit_transform(embeddings)\nplt.figure(figsize=(8, 6))\nplt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=5, alpha=0.7)\nplt.title(\"Node2Vec Embeddings (t-SNE projection)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T21:17:43.23988Z","iopub.execute_input":"2025-05-19T21:17:43.240238Z","iopub.status.idle":"2025-05-19T21:18:39.004107Z","shell.execute_reply.started":"2025-05-19T21:17:43.240206Z","shell.execute_reply":"2025-05-19T21:18:39.002848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## K-Means Clustering","metadata":{}},{"cell_type":"code","source":"# K-Means Clustering on Node2Vec Embeddings\nfrom sklearn.cluster import KMeans\n\n# Use previously computed Node2Vec embeddings\nn_clusters = 5\nkmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE)\nlabels = kmeans.fit_predict(embeddings)\n\nplt.figure(figsize=(8, 6))\nfor i in range(n_clusters):\n    plt.scatter(embeddings_2d[labels == i, 0], embeddings_2d[labels == i, 1], s=5, label=f'Cluster {i}')\nplt.title(\"K-Means Clustering on Node2Vec Embeddings\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T21:18:39.488978Z","iopub.execute_input":"2025-05-19T21:18:39.489281Z","iopub.status.idle":"2025-05-19T21:18:39.809689Z","shell.execute_reply.started":"2025-05-19T21:18:39.489253Z","shell.execute_reply":"2025-05-19T21:18:39.808824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Categorization of unlabeled samples","metadata":{}},{"cell_type":"code","source":"# Categorization of unlabeled samples using trained GNN\n# Predict classes for unknown nodes\nunknown_pred = predict(model, data)[unknown_mask]\nunknown_pred_labels = le.inverse_transform(unknown_pred.cpu().numpy())\n\n# Count predicted classes for unknown nodes\nunique, counts = np.unique(unknown_pred_labels, return_counts=True)\nprint(\"Predicted class distribution for unlabeled samples:\")\nfor label, count in zip(unique, counts):\n    print(f\"{label}: {count}\")\n\n# Optionally, assign predictions back to the original DataFrame\nelliptic_txs_classes.loc[unknown_mask.cpu().numpy(), 'predicted_class'] = unknown_pred_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T21:19:10.280691Z","iopub.execute_input":"2025-05-19T21:19:10.281045Z","iopub.status.idle":"2025-05-19T21:19:10.552712Z","shell.execute_reply.started":"2025-05-19T21:19:10.281013Z","shell.execute_reply":"2025-05-19T21:19:10.551736Z"}},"outputs":[],"execution_count":null}]}